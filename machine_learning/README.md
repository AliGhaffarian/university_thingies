This repository contains a pipeline for:
1. tracing syscalls of selected user-space programs using an eBPF tracer
2. formatting those traces into per-process syscall sequences
3. converting an existing labelled ADFA-LD syscall dataset into a PyTorch dataset
4. training / running a simple RNN classifier on syscall sequences to detect attacks vs normal behavior.

This README explains how the pieces fit together, how to build and run them, and documents internal data flows and implementation details so you can maintain, extend, or debug the project.

# Table of Content
- [Project Contents (High Level)](#project-contents-high-level)
- [Quick start (recommended order)](#quick-start-recommended-order)
- [File-by-file explanation & internals](#file-by-file-explanation-internals)
   * [`tracer/` (BPF tracer + loader)](#tracer-bpf-tracer-loader)
   * [Data pipeline & dataset scripts](#data-pipeline-dataset-scripts)
      + [`scripts/make_pytorch_dataset_dir_from_adfa.sh`](#scriptsmake_pytorch_dataset_dir_from_adfash)
      + [`scripts/longest_element.sh`](#scriptslongest_elementsh)
      + [`syscall_dataset.py`](#syscall_datasetpy)
   * [Mapping helpers: `syscall_nr_to_str.py`](#mapping-helpers-syscall_nr_to_strpy)
   * [Model: `model.py` (RNN classifier)](#model-modelpy-rnn-classifier)
      + [Architecture](#architecture)
      + [Data handling](#data-handling)
   * [Inference: `check_log.py`](#inference-check_logpy)


# Project Contents (High Level)

- `tracer/`: eBPF tracing code and loader/logger user program( [README here](<tracer/README.md>) )
- `scripts/`: dataset conversion utilities for ADFA-LD -> pytorch
  - `variables.sh`, `make_pytorch_dataset_dir_from_adfa.sh`, `longest_element.sh`, `duplicates.sh`
- `syscall_*`: mapping and dataset helper code
  - `syscall_nr_to_str.py`: mapping between x86_64 linux syscall numbers and the datasetâ€™s ID space
  - `syscall_dataset.py`: PyTorch `Dataset` implementation used by training code
- Model and tooling
  - `model.py`: RNN classifier and training loop
  - `check_log.py`: load trained model & classify a single formatted trace
- Top-level helpers
  - `trace.sh`: wrapper to build & run tracer (`sudo`) and run `formatter.py`
  - `Makefile`: wrapper to do some predefined operations (format ADFA-LD dataset, trace a program, etc)

# Quick start (recommended order)

1. Prepare dataset (if you have ADFA-LD):

   - Edit `scripts/variables.sh` and set `DATASET_DIR` to where the ADFA-LD dataset is located.
   - Run:

     ```bash
     make dataset
     ```

     This runs `scripts/make_pytorch_dataset_dir_from_adfa.sh` (copies files into `dataset/{training,testing}` and builds labels CSVs) and `scripts/longest_element.sh` (computes maximum sequence length and writes it to file `longest_elem`).

2. Trace a running program (requires sudo):

   - (Build the tracer if not built and) Run the tracer for a program name (exact program name as seen in the process comm or set `ALL` to trace everything):

     ```bash
     sudo make trace PROG_NAME=<PROGRAM_NAME>
     ```

   - Stop tracing with `Ctrl+C`. The tracer writes `trace.log` and `trace.sh` runs `formatter.py` to break the `trace.log` into per-(comm,pid) files named `<comm>:<pid>.log`.

3. (If missing) Train the model:

   - From project root:

     ```bash
     make rnn_syscall_classifier.pt
     ```
   - The model saves `rnn_syscall_classifier.pt`.

4. Classify a formatted trace:
   - Use `check_log.py` to classify a single formatted trace file produced by the formatter (`<comm>:<pid>.log`):

     ```bash
     make check_log LOG_FILENAME=<LOG_FILENAME>
     ```

# File-by-file explanation & internals

## `tracer/` (BPF tracer + loader)
[Readme here](<tracer/README.md>)
## Data pipeline & dataset scripts

### `scripts/make_pytorch_dataset_dir_from_adfa.sh`

Converts ADFA-LD dataset layout into a `dataset/` directory with `training/` and `testing/` subdirectories and creates CSVs `labels_training.csv` and `labels_testing.csv`.

### `scripts/longest_element.sh`

- Scans every file under `DATASET_DIR` and computes maximum sequence length (by counting spaces/tabs), writes the maximum to `longest_elem`.
- This `longest_elem` is used by `syscall_dataset.py` for padding sequences to a fixed length.

### `syscall_dataset.py`

- Implements `SyscallSequenceDataset` (PyTorch `Dataset`).
- Reads path to sequences from `syscall_seq_dir/(training|testing)/<filename>` and labels from a CSV (`annotations_file`).

**Notes about format**
- The per-process files generated by the formatter are single-line space-separated syscall IDs, matching the dataset format expected by the dataset loader.

## Mapping helpers: `syscall_nr_to_str.py`

- `dataset_syscall_id_map`: mapping `dataset_id -> syscall_name` used to map dataset indices to names.
- `syscall_name_to_nr_standard_linux`: mapping syscall name -> x86_64 Linux syscall number (a separate map).
- The module builds `standard_linux_to_dataset_map`: transforms x86_64 Linux syscall numbers into dataset IDs (or `PAD_VALUE` if the syscall isn't present in the dataset mapping).
- `PAD_VALUE` is `-1` (used for padding). When sequences are prepared for the embedding layer in the model, code often shifts them by `+1` so that padding index becomes `0` for `nn.Embedding(padding_idx=0)` compatibility.

## Model: `model.py` (RNN classifier)
### Architecture
- Single-layer `nn.RNN`.
- Final `Linear(hidden_dim -> num_classes)` outputs logits.
- Uses `pack_padded_sequence` so the RNN ignores padded timesteps.

### Data handling
- `PAD_VALUE` is `-1` in the dataset. Before feeding to the embedding, the code adds `+1` to every token, shifting padding `-1` -> `0` which matches `padding_idx=0` in `nn.Embedding`.
- During dataset creation the sequences are fixed to `longest_elem_size` length via padding.
## Inference: `check_log.py`

- Loads `rnn_syscall_classifier.pt` checkpoint (expected to include `model_state_dict` and `config`).
- Reconstructs the model, loads weights, reads a single formatted trace file (space-separated syscall numbers), converts x86_64 Linux numbers -> dataset IDs using `standard_linux_to_dataset_map` (falling back to `PAD_VALUE` if unknown), shifts by +1, and infers.
- Prints label using `syscall_dataset.labels_map` (0:normal, 1:attack).

**Note:** Since the formatted traces are already dataset IDs when generated by the `formatter.py` if used together with the tracer, `check_log.py` expects sequences in dataset ID space. If you have a trace containing x86_64 Linux syscall numbers, the mapping `standard_linux_to_dataset_map` is applied.
